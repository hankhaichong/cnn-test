{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import struct as st\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(image):\n",
    "    image = 255 - image.astype('float64')\n",
    "    image = np.interp(image, (image.min(), image.max()), (-1, +1))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kernel(size):\n",
    "    \n",
    "    \"\"\"\n",
    "    Width and height of the kernel where width = height\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.random.normal(scale = 1.0, size = size**2)\n",
    "    # to ensure the initialization number is not too big\n",
    "    f = lambda x: x if abs(x) < 1 else x/2\n",
    "    try:\n",
    "        while np.any(abs(y) > 1):\n",
    "            y = np.array(tuple(map(f,y)))\n",
    "    except:\n",
    "        y = np.array(tuple(map(f,x)))\n",
    "        \n",
    "    return y.reshape(size,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights_bias(input_size, output_size):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create weights and bias for fully connected layer. \n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.random.normal(scale = 1.0, size = input_size*output_size)\n",
    "    b = np.random.normal(scale = 1.0, size = output_size)\n",
    "    # to ensure the initialization number is not too big\n",
    "    f = lambda x: x if abs(x) < 1 else x/2\n",
    "    \n",
    "    try:\n",
    "        while np.any(abs(y) > 1):\n",
    "            y = np.array(tuple(map(f,y)))\n",
    "    except:\n",
    "        y = np.array(tuple(map(f,x)))\n",
    "        \n",
    "    try:\n",
    "        while np.any(abs(z) > 1):\n",
    "            z = np.array(tuple(map(f,z)))\n",
    "    except:\n",
    "        z = np.array(tuple(map(f,b)))\n",
    "        \n",
    "    return {\"weights\": y.reshape(input_size,output_size),\n",
    "           \"bias\": z}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn_forward(inputs, weight_bias):\n",
    "    \n",
    "    \"\"\"\n",
    "    Feed forward for fully connected layers\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = weight_bias['weights']\n",
    "    bias = weight_bias['bias']\n",
    "    output = np.dot(inputs, weights)+bias\n",
    "    return output\n",
    "\n",
    "def fcn_backward(backprop_prev, X, weight_bias, lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Backpropagation for fully connected layers, inputs required are:\n",
    "    1. Gradient with respect to loss from previous layer\n",
    "    2. The inputs in the feed forward layer.\n",
    "    3. Weights and bias in this fully connected layer\n",
    "    4. Learning Rate\n",
    "    \n",
    "    Return:\n",
    "    1. Gradient with respect to loss of weights, inputs and bias.\n",
    "    2. Updated weights and bias for the next batch of data.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = np.array([X])\n",
    "    weights = weight_bias['weights']\n",
    "    bias = weight_bias['bias']\n",
    "    backprop_prev_T = np.array([backprop_prev]).T\n",
    "    dw = np.dot(backprop_prev_T, X).T\n",
    "    weights_T = weights.T\n",
    "    dx = np.dot(backprop_prev, weights_T)\n",
    "    db = backprop_prev\n",
    "    \n",
    "    weights -= lr*dw\n",
    "    bias -= lr*db\n",
    "    \n",
    "    return {\"dw\": dw,\n",
    "           \"dx\":dx,\n",
    "           \"db\": db,\n",
    "           \"fcn_weight_bias\": {\n",
    "               \"weights\": weights,\n",
    "               \"bias\": bias\n",
    "           }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(X, kernel):\n",
    "    \n",
    "    \"\"\"\n",
    "    Feed forward for convolution layer.\n",
    "    Current limitation: No striding and padding\n",
    "    \"\"\"\n",
    "    \n",
    "    # assume only square images and square kernels\n",
    "    input_size = X.shape[0]\n",
    "    kernel_size = kernel.shape[0]\n",
    "    \n",
    "    # assume stride 1 with no padding\n",
    "    output_size = input_size - kernel_size + 1\n",
    "    output = np.zeros((output_size, output_size))\n",
    "    \n",
    "    for height in range(output_size):\n",
    "        for width in range(output_size):\n",
    "            input_slice = X[height:height+kernel_size, width:width+kernel_size]\n",
    "            output[height, width] = np.sum(np.multiply(input_slice, kernel))\n",
    "    \n",
    "    return output\n",
    "\n",
    "def conv_backprop(backprop_prev, X, kernel, lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Backpropagation of convolution layer, inputs required are:\n",
    "    1. Gradient with respect to loss from previous layer (in the conv_forward output shape)\n",
    "    2. The inputs in the feed forward layer.\n",
    "    3. Kernel in this convolution layer.\n",
    "    4. Learning Rate\n",
    "    \n",
    "    Return:\n",
    "    1. Gradient with respect to loss of weights and inputs.\n",
    "    2. Updated kernel for the next batch of data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # getting the size of the backprop from previous gradient\n",
    "    # assuming square\n",
    "    input_size = X.shape[0]\n",
    "    backprop_prev_size = backprop_prev.shape[0]\n",
    "    kernel_size = kernel.shape[0]\n",
    "    dw = np.zeros((kernel_size, kernel_size))\n",
    "    dx = np.zeros((input_size, input_size))\n",
    "    \n",
    "    for h in range(backprop_prev_size):\n",
    "        for w in range(backprop_prev_size):\n",
    "            dx[h:h+kernel_size, w:w+kernel_size] += kernel*backprop_prev[h,w]\n",
    "            dw += X[h:h+kernel_size, w: w+kernel_size]*backprop_prev[h,w]\n",
    "            \n",
    "    kernel -= lr*dw\n",
    "    \n",
    "    return {\"dw\": dw, \"dx\": dx, \"kernel\": kernel} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool_forward(X, pool_size, stride):\n",
    "    \n",
    "    \"\"\"\n",
    "    Feed forward of maxpooling.\n",
    "    Current limitation: No padding.\n",
    "    Requires input, size of pooling and stride.\n",
    "    \n",
    "    Returns output after maxpooling and the input index for each of the output (to be used later during backprop)\n",
    "    \"\"\"\n",
    "    \n",
    "    # assume no padding\n",
    "    input_size = X.shape[0]\n",
    "    \n",
    "    # assume no padding\n",
    "    output_size = math.ceil((input_size - pool_size)/stride + 1)\n",
    "    output = np.zeros((output_size, output_size))\n",
    "    max_index = np.empty((output_size, output_size), dtype = object)\n",
    "    grad_matrix = np.zeros((input_size, input_size))\n",
    "    \n",
    "    for height in range(output_size):\n",
    "        for width in range(output_size):\n",
    "            input_slice = X[height*stride:(height*stride+pool_size), \n",
    "                            width*stride:(width*stride+pool_size)]\n",
    "            output[height, width] = np.max(input_slice)\n",
    "            local_max_location = np.unravel_index(np.argmax(input_slice, axis = None), input_slice.shape)\n",
    "            max_index[height, width] = np.asarray(local_max_location)+[height*stride,width*stride]\n",
    "            \n",
    "#     max_index = max_index.flatten()\n",
    "    \n",
    "#     for index in max_index:\n",
    "#         grad_matrix[index[0], index[1]] += 1.00\n",
    "            \n",
    "    return {\"maxpool_input\": X, \"maxpool_output\": output, \"max_index_location\": max_index}\n",
    "\n",
    "\n",
    "def maxpool_backprop(input_size, backprop_prev, max_index_location):\n",
    "    \n",
    "    \"\"\"\n",
    "    Backpropagation for the maxpool layer.\n",
    "    Require:\n",
    "    1. The size of the input when passed throught this layer.\n",
    "    2. The gradient flowing from the previous layer.\n",
    "    3. The index of the input that contributed to the output of the maxpool layer.\n",
    "    \"\"\"\n",
    "\n",
    "    backprop_prev_size = backprop_prev.shape[0]\n",
    "    maxpool_grad = np.zeros((input_size, input_size))\n",
    "    \n",
    "    for height in range(backprop_prev_size):\n",
    "        for weight in range(backprop_prev_size):\n",
    "            max_loc = max_index_location[height, weight]\n",
    "            maxpool_grad[max_loc[0], max_loc[1]] += backprop_prev[height, weight]\n",
    "            \n",
    "    return maxpool_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(input_array):\n",
    "    \n",
    "    \"\"\"\n",
    "    Turns any inputs that are negative to zero.\n",
    "    Returns 1 for index of the inputs that are non-zero after RELU for backpropagation purpose.\n",
    "    \"\"\"\n",
    "    \n",
    "    shape = input_array.shape\n",
    "    grad_matrix = np.zeros(shape)\n",
    "    input_array[input_array < 0] = 0\n",
    "    nonzero_location = np.asarray(input_array.nonzero()).T\n",
    "    for index in nonzero_location:\n",
    "        grad_matrix[index[0], index[1]] = 1.00\n",
    "        \n",
    "    return grad_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_softmax(input_vector):\n",
    "    \n",
    "    \"\"\"\n",
    "    Refer to https://deepnotes.io/softmax-crossentropy\n",
    "    \"\"\"\n",
    "    \n",
    "    exps = np.exp(input_vector - np.max(input_vector))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def cross_entropy_loss(pred_array, actual):\n",
    "    \n",
    "    pred_prob = pred_array[actual]\n",
    "    loss_log_likelihood = -np.log(pred_prob)\n",
    "    \n",
    "    return loss_log_likelihood\n",
    "\n",
    "\n",
    "def delta_cross_entropy(X,y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Refer to https://deepnotes.io/softmax-crossentropy\n",
    "    Returns loss and gradient up to the softmax layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    grad = stable_softmax(X)\n",
    "    pred = grad.argmax()\n",
    "    loss = cross_entropy_loss(grad,y)\n",
    "    grad[y] -= 1\n",
    "    return {\"loss\": loss,\n",
    "           \"gradient\": grad,\n",
    "           \"pred\": pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input, weights = None):\n",
    "    \n",
    "    if weights is None:\n",
    "        # Weights, kernels and bias initialization\n",
    "        conv1_kernel = create_kernel(3)\n",
    "        conv2_kernel = create_kernel(2)\n",
    "        fcn1_weight_bias = create_weights_bias(36, 28)\n",
    "        fcn2_weight_bias = create_weights_bias(28,10)\n",
    "    \n",
    "    else:\n",
    "        conv1_kernel = weights['conv1_kernel']\n",
    "        conv2_kernel = weights['conv2_kernel']\n",
    "        fcn1_weight_bias = weights['fcn1_weight_bias']\n",
    "        fcn2_weight_bias = weights['fcn2_weight_bias']\n",
    "    \n",
    "    #training for loop\n",
    "    for counter, image_label in enumerate(input):\n",
    "        label = image_label['label']\n",
    "        input_image = image_label['images']\n",
    "        # first layer set\n",
    "        x1 = conv_forward(input_image, conv1_kernel)\n",
    "        maxpool1_size = x1.shape[0]\n",
    "        relu_1 = relu(x1)\n",
    "        y1 = maxpool_forward(x1,2,2)\n",
    "        x1 = y1['maxpool_output']\n",
    "        \n",
    "        # second layer set\n",
    "        x2 =  conv_forward(x1, conv2_kernel)\n",
    "        maxpool2_size = x2.shape[0]\n",
    "        relu_2 = relu(x2)\n",
    "        y2 = maxpool_forward(x2,2,2)\n",
    "        x2 = y2['maxpool_output']\n",
    "        \n",
    "        # third layer set\n",
    "        x3 = x2.flatten() # flatten into one d array\n",
    "        x4 = fcn_forward(x3, fcn1_weight_bias)\n",
    "        x5 = fcn_forward(x4, fcn2_weight_bias)\n",
    "        \n",
    "        z = delta_cross_entropy(x5, label)\n",
    "        print('Image ' + str(counter) + ' loss: ' + str(z['loss']) + '. predicted: ' + str(z['pred']) + \n",
    "              ', actual: ' + str(label))\n",
    "        \n",
    "        fcn2_backprop = fcn_backward(z['gradient'], x4, fcn2_weight_bias, 0.0001)\n",
    "        x5_backprop, fcn2_weight_bias = fcn2_backprop['dx'], fcn2_backprop['fcn_weight_bias']\n",
    "        \n",
    "        fcn1_backprop = fcn_backward(x5_backprop, x3, fcn1_weight_bias, 0.0001)\n",
    "        x4_backprop, fcn1_weight_bias = fcn1_backprop['dx'], fcn1_backprop['fcn_weight_bias']\n",
    "        \n",
    "        x3_backprop = x4_backprop.reshape(x2.shape)\n",
    "        x2_maxpool_backprop = maxpool_backprop(maxpool2_size, x3_backprop, y2['max_index_location'])\n",
    "        x2_relu_backprop = np.multiply(x2_maxpool_backprop, relu_2)\n",
    "        x2_conv_backprop = conv_backprop(x2_relu_backprop, x1, conv2_kernel, 0.0001)\n",
    "        x2_backprop, conv2_kernel = x2_conv_backprop['dx'], x2_conv_backprop['kernel']\n",
    "        \n",
    "        x1_maxpool_backprop = maxpool_backprop(maxpool1_size, x2_backprop, y1['max_index_location'])\n",
    "        x1_relu_backprop = np.multiply(x1_maxpool_backprop, relu_1)\n",
    "        x1_conv_backprop = conv_backprop(x1_relu_backprop, input_image, conv1_kernel, 0.0001)\n",
    "        conv1_kernel = x1_conv_backprop['kernel']\n",
    "    \n",
    "    return {\n",
    "        \"conv1_kernel\": conv1_kernel,\n",
    "        \"conv2_kernel\": conv2_kernel,\n",
    "        \"fcn1_weight_bias\": fcn1_weight_bias,\n",
    "        \"fcn2_weight_bias\": fcn2_weight_bias\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = {'images' : './data/raw/train-images-idx3-ubyte' ,'labels' : './data/raw/train-labels-idx1-ubyte'}\n",
    "train_imagesfile = open(filename['images'],'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imagesfile.seek(0)\n",
    "magic = st.unpack('>4B',train_imagesfile.read(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Img = st.unpack('>I',train_imagesfile.read(4))[0] #num of images\n",
    "nR = st.unpack('>I',train_imagesfile.read(4))[0] #num of rows\n",
    "nC = st.unpack('>I',train_imagesfile.read(4))[0] #num of column\n",
    "images_array = np.zeros((Img,nR,nC))\n",
    "nBytesTotal = Img*nR*nC*1 #since each pixel data is 1 byte\n",
    "images_array = 255 - np.asarray(st.unpack('>'+'B'*nBytesTotal,train_imagesfile.read(nBytesTotal))).reshape((Img,nR,nC))\n",
    "train_imagesfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = open(filename['labels'], 'rb')\n",
    "train_label.read(8)\n",
    "train_label_list = []\n",
    "for i in range(20):\n",
    "    label = ord(train_label.read(1))\n",
    "    train_label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_label_list)):\n",
    "    image = rescale(images_array[i])\n",
    "    input_dict = {\"images\": image,\n",
    "                 \"label\": train_label_list[i]}\n",
    "    input_dict_list.append(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = train(input_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    parameters = train(input_dict_list, parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
